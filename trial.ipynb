{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.7' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '\"c:/Users/ANANTHA KRISHNA S K/AppData/Local/Microsoft/WindowsApps/python3.12.exe\" -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.7' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '\"c:/Users/ANANTHA KRISHNA S K/AppData/Local/Microsoft/WindowsApps/python3.12.exe\" -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "os.environ[\"GROQ_API_KEY\"] = \"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\"\n",
    "os.environ[\"PINECONE_API_KEY\"] = \"a7c62fa0-36d5-489c-83a1-198d568ce0be\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fast language models, also known as rapid language models or quick language models, are a type of artificial intelligence (AI) system designed to process and generate human-like language at a remarkably high speed. They are a critical component of many natural language processing (NLP) applications, and their importance cannot be overstated. Here are some reasons why:\n",
      "\n",
      "1. **Real-time processing**: Fast language models enable real-time processing of large amounts of language data, allowing for instant responses, recommendations, or completions. This is crucial for applications like chatbots, language translation, and text-based customer service.\n",
      "2. **Scalability**: With the ability to process vast amounts of text data quickly, fast language models can handle massive volumes of user input, such as search queries, forum comments, or social media posts. This scalability is essential for large-scale NLP applications.\n",
      "3. **Efficient inference**: Fast language models can perform inference (i.e., predicting the output given the input) quickly, which is critical for applications that require fast decision-making, such as sentiment analysis, spam detection, or language translation.\n",
      "4. **Improved user experience**: By generating responses quickly, fast language models can provide a faster and more responsive user experience, which is essential for applications like instant messaging, email clients, or online search engines.\n",
      "5. **Flexibility**: Fast language models can be used in a wide range of applications, from simple text processing tasks (e.g., spell checking, grammar correction) to complex tasks (e.g., machine translation, text summarization, dialogue generation).\n",
      "6. **Competitive advantage**: The ability to process language quickly and efficiently can give organizations a competitive edge in various industries, such as:\n",
      "\t* Customer service: providing rapid responses to customer queries\n",
      "\t* Marketing: generating personalized and targeted content quickly\n",
      "\t* Research: analyzing large volumes of text data rapidly\n",
      "7. **Enabling new applications**: Fast language models have the potential to enable new applications and use cases, such as:\n",
      "\t* Conversational AI: rapid responses to user input\n",
      "\t* Virtual assistants: quick and accurate language understanding\n",
      "\t* Language-based games: fast text processing for real-time gameplay\n",
      "8. **Advancements in other AI areas**: The development of fast language models has also led to advancements in other AI areas, such as computer vision, speech recognition, and machine learning.\n",
      "\n",
      "In summary, fast language models are essential for applications that require rapid processing, scalability, and efficient inference of large amounts of language data. Their importance extends beyond traditional NLP tasks and has far-reaching implications for various industries and applications.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from groq import Groq\n",
    "\n",
    "client = Groq(\n",
    "    # This is the default and can be omitted\n",
    "    api_key=os.environ.get(\"GROQ_API_KEY\"),\n",
    ")\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"you are a helpful assistant.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Explain the importance of fast language models\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"llama3-8b-8192\",\n",
    ")\n",
    "\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "pc = Pinecone(api_key=\"75ee75fc-ed27-4292-8e81-56cd524452ec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"quickstart\"\n",
    "\n",
    "pc.create_index(\n",
    "    name=index_name,\n",
    "    dimension=1024, # Replace with your model dimensions\n",
    "    metric=\"cosine\", # Replace with your model metric\n",
    "    spec=ServerlessSpec(\n",
    "        cloud=\"aws\",\n",
    "        region=\"us-east-1\"\n",
    "    ) \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'values': [0.04925537109375, -0.01313018798828125, ..., -0.01971435546875, -0.01107025146484375]}\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    {\"id\": \"vec1\", \"text\": \"Apple is a popular fruit known for its sweetness and crisp texture.\"},\n",
    "    {\"id\": \"vec2\", \"text\": \"The tech company Apple is known for its innovative products like the iPhone.\"},\n",
    "    {\"id\": \"vec3\", \"text\": \"Many people enjoy eating apples as a healthy snack.\"},\n",
    "    {\"id\": \"vec4\", \"text\": \"Apple Inc. has revolutionized the tech industry with its sleek designs and user-friendly interfaces.\"},\n",
    "    {\"id\": \"vec5\", \"text\": \"An apple a day keeps the doctor away, as the saying goes.\"},\n",
    "    {\"id\": \"vec6\", \"text\": \"Apple Computer Company was founded on April 1, 1976, by Steve Jobs, Steve Wozniak, and Ronald Wayne as a partnership.\"}\n",
    "]\n",
    "\n",
    "embeddings = pc.inference.embed(\n",
    "    model=\"multilingual-e5-large\",\n",
    "    inputs=[d['text'] for d in data],\n",
    "    parameters={\"input_type\": \"passage\", \"truncate\": \"END\"}\n",
    ")\n",
    "print(embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'upserted_count': 6}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Wait for the index to be ready\n",
    "while not pc.describe_index(index_name).status['ready']:\n",
    "    time.sleep(1)\n",
    "\n",
    "index = pc.Index(index_name)\n",
    "\n",
    "vectors = []\n",
    "for d, e in zip(data, embeddings):\n",
    "    vectors.append({\n",
    "        \"id\": d['id'],\n",
    "        \"values\": e['values'],\n",
    "        \"metadata\": {'text': d['text']}\n",
    "    })\n",
    "\n",
    "index.upsert(\n",
    "    vectors=vectors,\n",
    "    namespace=\"ns1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dimension': 1024,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {},\n",
      " 'total_vector_count': 0}\n"
     ]
    }
   ],
   "source": [
    "print(index.describe_index_stats())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"give me the detailed code with steps for creating a new index in pinecone\"\n",
    "\n",
    "embedding = pc.inference.embed(\n",
    "    model=\"multilingual-e5-large\",\n",
    "    inputs=[query],\n",
    "    parameters={\n",
    "        \"input_type\": \"query\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'matches': [], 'namespace': 'ns1', 'usage': {'read_units': 1}}\n"
     ]
    }
   ],
   "source": [
    "results = index.query(\n",
    "    namespace=\"ns1\",\n",
    "    vector=embedding[0].values,\n",
    "    top_k=3,\n",
    "    include_values=False,\n",
    "    include_metadata=True\n",
    ")\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for result in results['matches']:\n",
    "    print(result['metadata']['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-17 15:58:47.695 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-17 15:58:47.699 WARNING streamlit.runtime.state.session_state_proxy: Session state does not function when running a script without `streamlit run`\n",
      "2024-10-17 15:58:47.701 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-17 15:58:47.703 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-17 15:58:47.704 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-17 15:58:48.750 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run C:\\Users\\ANANTHA KRISHNA S K\\anaconda3\\Lib\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n",
      "2024-10-17 15:58:48.753 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-17 15:58:48.753 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-17 15:58:48.755 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-17 15:58:48.755 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-17 15:58:48.755 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-17 15:58:48.766 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-17 15:58:48.767 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-17 15:58:48.770 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-17 15:58:48.771 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "import streamlit as st\n",
    "import base64\n",
    "from groq import Groq\n",
    "\n",
    "client = Groq(\n",
    "    # This is the default and can be omitted\n",
    "    api_key=os.environ.get(\"GROQ_API_KEY\"),\n",
    ")\n",
    "\n",
    "st.session_state.api_key = os.environ.get(\"GROQ_API_KEY\")\n",
    "\n",
    "# Only show the API key input if the key is not already set\n",
    "if not st.session_state.api_key:\n",
    "    # Ask the user's API key if it doesn't exist\n",
    "    api_key = st.text_input(\"Enter API Key\", type=\"password\")\n",
    "    \n",
    "    # Store the API key in the session state once provided\n",
    "    if api_key:\n",
    "        st.session_state.api_key = api_key\n",
    "        st.rerun()  # Refresh the app once the key is entered to remove the input field\n",
    "else:\n",
    "    # If the API key exists, show the chat app\n",
    "    st.title(\"Chat App\")\n",
    "\n",
    "    # Initialize the chat message list in session state if it doesn't exist\n",
    "    if \"chat_messages\" not in st.session_state:\n",
    "        st.session_state.chat_messages = []\n",
    "\n",
    "    # Display previous chat messages\n",
    "    for messages in st.session_state.chat_messages:\n",
    "        if messages[\"role\"] in [\"user\", \"assistant\"]:\n",
    "            with st.chat_message(messages[\"role\"]):\n",
    "                st.markdown(messages[\"content\"])\n",
    "    \n",
    "    # Define a function to simulate chat interaction (you would replace this with an actual API call)\n",
    "    def get_chat():\n",
    "        embedding = pc.inference.embed(\n",
    "            model=\"multilingual-e5-large\",\n",
    "            inputs=[st.session_state.chat_messages[-1][\"content\"]],\n",
    "            parameters={\n",
    "                \"input_type\": \"query\"\n",
    "            }\n",
    "        )\n",
    "        results = index.query(\n",
    "            namespace=\"ns1\",\n",
    "            vector=embedding[0].values,\n",
    "            top_k=3,\n",
    "            include_values=False,\n",
    "            include_metadata=True\n",
    "        )\n",
    "        context = \"\"\n",
    "        for result in results['matches']:\n",
    "            context += result['metadata']['text'] + \"\\n\"\n",
    "            \n",
    "        chat_completion = client.chat.completions.create(\n",
    "            messages=st.session_state.chat_messages,\n",
    "            model=\"llama3-8b-8192\",\n",
    "        )\n",
    "        return chat_completion.choices[0].message.content\n",
    "\n",
    "    # Handle user input\n",
    "    if prompt := st.chat_input(\"Try asking the bot what it can do, or thank it for its help!\"):\n",
    "        # Display user message\n",
    "        with st.chat_message(\"user\"):\n",
    "            st.markdown(prompt)\n",
    "        st.session_state.chat_messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "        # Get the assistant's response (in this case, it's just echoing the prompt)\n",
    "        with st.spinner(\"Getting responses...\"):\n",
    "            response = get_chat()\n",
    "        with st.chat_message(\"assistant\"):\n",
    "            st.markdown(response)\n",
    "        \n",
    "        # Add user message and assistant response to chat history\n",
    "        st.session_state.chat_messages.append({\"role\": \"assistant\", \"content\": response})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.7' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '\"c:/Users/ANANTHA KRISHNA S K/AppData/Local/Microsoft/WindowsApps/python3.12.exe\" -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load your dataset from the local directory\n",
    "new_file_path = 'D:/CODE BOT 2.0/data set/Crop_recommendation.csv'\n",
    "crop_data = pd.read_csv(new_file_path)\n",
    "\n",
    "# Drop the 'label' column as it's the target\n",
    "features = crop_data.drop(columns=['label']).values\n",
    "\n",
    "# Normalize the features\n",
    "scaler = MinMaxScaler()\n",
    "normalized_features = scaler.fit_transform(features)\n",
    "\n",
    "# Now you have normalized data ready for embedding into Pinecone\n",
    "print(normalized_features[:5])  # Display the first few normalized vectors for verification\n",
    "\n",
    "#Re-load the dataset to ensure we're working with the latest upload\n",
    "new_file_path = 'D:\\CODE BOT 2.0\\data set\\Crop_recommendation.csv'\n",
    "crop_data = pd.read_csv(new_file_path)\n",
    "\n",
    "# Drop the label column as it's the target\n",
    "features = crop_data.drop(columns=['label']).values\n",
    "\n",
    "# Normalize the features for embedding into Pinecone\n",
    "scaler = MinMaxScaler()\n",
    "normalized_features = scaler.fit_transform(features)\n",
    "\n",
    "# Now we have the normalized data which is ready for embedding into Pinecone\n",
    "normalized_features[:5]  # Display the first few normalized vectors for verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
